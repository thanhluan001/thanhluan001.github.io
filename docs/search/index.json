[{"content":" Today is the first day of the rest of your life.\n— Charles Dederich\n Here is the plan for next tutorials: TODO\n","href":"/","title":"Home"},{"content":"","href":"/posts/","title":"All Blog Posts"},{"content":"Plan of things to come :)\n Chatbot with Rasa. Explanation of using Transformer in chatbot Times series with Phophet :) NLP tutorial part 2 (with CNN and RNN) NLP tutorial part 1 (here)  ","href":"/todo/","title":"ToDo"},{"content":"Here is a paragraph. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nHeading 2 Another one. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nHeading 3 Yet another, but centered! Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.  Heading 4  First item Second item   Nested unordered item  Third item Nested ordered item 1 Nested ordered item 2  Heading 5 Where are the quotes!!!\n Simplify, then add lightness.\n— Colin Chapman\n Now, time for some links!\n GoHugo Hugo Themes  Heading 6 Inline code: echo \u0026quot;What is the meaning of life?\u0026quot;. Who knows?\n// Codeblock  var meaningOfLife = 42; console.log(\u0026#39;The meaning of life is: \u0026#39;, meaningOfLife);  Who wants some table?\n   Minimo Caption More Caption     Cool What? Now, wut?!    Ah, enough for today, eh?\n","href":"/typography/","title":"Typography"},{"content":"I am just a programmer who have learnt too many things over the years. This blog is for me to keep track of them :). If you find anything useful, please share.\n","href":"/about/","title":"About"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/tags/deep-learning/","title":"Deep Learning"},{"content":"","href":"/authors/luanpham/","title":"luanpham"},{"content":"","href":"/tags/machine-learning/","title":"Machine Learning"},{"content":"","href":"/tags/nlp/","title":"NLP"},{"content":"We follow a NLP tutorial with code today (this link)\nGetting Data We\u0026rsquo;re using \u0026ldquo;Disasters in Social Media\u0026rdquo; dataset which consists of tweet archive where\n Over 10,000 tweets culled with a variety of searches like “ablaze”, “quarantine”, and “pandemonium”, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).\n The objective is to distinguish between tweets that signal disasters and \u0026ldquo;irrelevant\u0026rdquo; tweets.\nProcess Data 1. Show Data A sample of the data:\ntext\tchoose_one\tclass_label 0\tJust happened a terrible car crash\tRelevant\t1 1\tOur Deeds are the Reason of this #earthquake M...\tRelevant\t1 2\tHeard about #earthquake is different cities, s...\tRelevant\t1 3\tthere is a forest fire at spot pond, geese are...\tRelevant\t1 4\tForest fire near La Ronge Sask. Canada\tRelevant\t1 5\tAll residents asked to 'shelter in place' are ...\tRelevant\t1 6\t13,000 people receive #wildfires evacuation or...\tRelevant\t1 7\tJust got sent this photo from Ruby #Alaska as ...\tRelevant\t1 8\t#RockyFire Update =\u0026gt; California Hwy. 20 closed...\tRelevant\t1 9\tApocalypse lighting. #Spokane #wildfires\tRelevant\t1 2. Clean Data Firstly, we to clean some data and replace all the extranious tokens with empty string \u0026quot;\u0026quot;\n# questions is the dataset loaded with pandas def standardize_text(df, text_field): df[text_field] = df[text_field].str.replace(r\u0026#34;http\\S+\u0026#34;, \u0026#34;\u0026#34;) df[text_field] = df[text_field].str.replace(r\u0026#34;http\u0026#34;, \u0026#34;\u0026#34;) df[text_field] = df[text_field].str.replace(r\u0026#34;@\\S+\u0026#34;, \u0026#34;\u0026#34;) df[text_field] = df[text_field].str.replace(r\u0026#34;[^A-Za-z0-9(),!?@\\\u0026#39;\\`\\\u0026#34;\\_\\n]\u0026#34;, \u0026#34; \u0026#34;) df[text_field] = df[text_field].str.replace(r\u0026#34;@\u0026#34;, \u0026#34;at\u0026#34;) df[text_field] = df[text_field].str.lower() return df questions = standardize_text(questions, \u0026#34;text\u0026#34;) questions.to_csv(\u0026#34;clean_data.csv\u0026#34;) questions.head() 3. Tokenize Data from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer(r\u0026#39;\\w+\u0026#39;) clean_questions[\u0026#34;tokens\u0026#34;] = clean_questions[\u0026#34;text\u0026#34;].apply(tokenizer.tokenize) Here we tokenizer by words separated by space r'\\w+'\nActual Machine Learning :D Bag of Words Count The simplest approach we can start with is to use a bag of words model, and apply a logistic regression on top. A bag of words just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence.\nfrom sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer def cv(data): count_vectorizer = CountVectorizer() emb = count_vectorizer.fit_transform(data) return emb, count_vectorizer list_corpus = clean_questions[\u0026#34;text\u0026#34;].tolist() list_labels = clean_questions[\u0026#34;class_label\u0026#34;].tolist() X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=40) X_train_counts, count_vectorizer = cv(X_train) X_test_counts = count_vectorizer.transform(X_test) X_train_counts is the new embeding with each word embeded in a vector of 15928 length (more info at this link). This approach totally ignores the order of words in our sentences.\nX_train_counts.get_shape() (8687, 15928) Visualize the embedding Normally we need to reduce the dimension of the embedding to graph\nBag of Words visualization\n As shown in the image, there is no distinguishing features between the Irrelevant and Disaster class because the vector from Bag of Word algorithm was initialized randomly.\nLinear Classifier Use a simple linear classifier as a baseline:\nfrom sklearn.linear_model import LogisticRegression clf = LogisticRegression(C=30.0, class_weight=\u0026#39;balanced\u0026#39;, solver=\u0026#39;newton-cg\u0026#39;, multi_class=\u0026#39;multinomial\u0026#39;, n_jobs=-1, random_state=40) clf.fit(X_train_counts, y_train) y_predicted_counts = clf.predict(X_test_counts) And use sklearn.metrics matric to measure the peformance:\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report def get_metrics(y_test, y_predicted): # true positives / (true positives+false positives) precision = precision_score(y_test, y_predicted, pos_label=None, average=\u0026#39;weighted\u0026#39;) # true positives / (true positives + false negatives) recall = recall_score(y_test, y_predicted, pos_label=None, average=\u0026#39;weighted\u0026#39;) # harmonic mean of precision and recall f1 = f1_score(y_test, y_predicted, pos_label=None, average=\u0026#39;weighted\u0026#39;) # true positives + true negatives/ total accuracy = accuracy_score(y_test, y_predicted) return accuracy, precision, recall, f1 We got accuracy of 0.761 and f1 score of 0.759 for a simple linear classifier. Not bad. This model is very simple to train and we can easily extract the most important features.\nConfusing matrix One of the most useful tool is the confusing matrix where we can have in-depth vision of where the model got wrong.\n As we can see, the model predicts 25% false negative, which mean it predicts \u0026ldquo;Irrelevant\u0026rdquo; when in fact it was a \u0026ldquo;Disaster\u0026rdquo;. False positive is not desirable in this problem because it gives the authority less time to prepare when in fact there is a disaster coming in.\nSome extra visualizations in the notebook, most notably most important words that the classifier uses to determine the class.\nTFIDF Bag of Words We use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise.\ndef tfidf(data): tfidf_vectorizer = TfidfVectorizer() train = tfidf_vectorizer.fit_transform(data) return train, tfidf_vectorizer X_train_tfidf, tfidf_vectorizer = tfidf(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) IF-IDF visualization\n Here the result is better. By leaving out many common words, it is easier to separate the 2 groups. Now our linear classifier should be able to do a better job. New accuracy is 76%, which is not a big improvement from previous method.\n Word2Vec Due to small number of tweets, it is unlikely that our model will pick up the semantic meaning of words. It means that some tweets are classfied differently even when they contain very similiar words. To rectify this problem, we use a Word2Vec to capture the closeness between words and use in our model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300): if len(tokens_list)\u0026lt;1: return np.zeros(k) if generate_missing: vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list] else: vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list] length = len(vectorized) summed = np.sum(vectorized, axis=0) averaged = np.divide(summed, length) return averaged def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False): embeddings = clean_questions[\u0026#39;tokens\u0026#39;].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing)) return list(embeddings) embeddings = get_word2vec_embeddings(word2vec, clean_questions) X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, test_size=0.2, random_state=40)   As we see in line 10, we do average of all word2vec vectors of each word. Just this simple calculation give us a quite good representation the tweets, even though order is not taken into consideration. We graph the new average word2vec as below\nAverage word2vec\n And the confusion matrix, new method has almost identical accuracy but is still a sligh improvement over all previous methods.\nAverage word2vec\n CNN for text classification Convolutional Neural Network (CNN) model is better known in image classification but it can be used in text classification. Unlike the previous models where order is not taken into consideration, CNN can distinguish between \u0026ldquo;Paul eats plants\u0026rdquo; and \u0026ldquo;Plants eat Paul\u0026rdquo;\nSee more in part 2\u0026hellip;\n","href":"/posts/nlp-learning/","title":"Nlp Learning with tutorial and code (part 1)"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/categories/tutorial/","title":"Tutorial"},{"content":"","href":"/tags/cuda/","title":"Cuda"},{"content":"Courtesy of this link for the information. This other link is an older answer.\nTo remove most all trace of cuda installations in your system\nsudo rm /etc/apt/sources.list.d/cuda* sudo apt remove --autoremove nvidia-cuda-toolkit sudo apt remove --autoremove nvidia-* Get the reposistory\nsudo add-apt-repository ppa:graphics-drivers sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo bash -c \u0026#39;echo \u0026#34;deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\u0026#34; \u0026gt; /etc/apt/sources.list.d/cuda.list\u0026#39; sudo bash -c \u0026#39;echo \u0026#34;deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\u0026#34; \u0026gt; /etc/apt/sources.list.d/cuda_learn.list\u0026#39; Install Cuda, no need to install libcudnn (unsure why, can be that the libcudnn is included in new .deb package)\nsudo apt update sudo apt install cuda As of this writing, it is Cuda 11.1. You can specify the version number like cuda-10-1 for Cuda 10.1. Check the repo for update-to-date packages\nCheck your installation of cuda by using pytorch\nimport torch torch.cuda.is_available() True ","href":"/posts/install-cuda-11-in-ubuntu/","title":"Install Cuda 11 in Ubuntu"},{"content":"","href":"/tags/installation/","title":"Installation"},{"content":"","href":"/tags/helloworld/","title":"helloworld"},{"content":"A Hello World example here is appropriate :)\nprint(\u0026#39;Hello World\u0026#39;); ","href":"/posts/my-first-post/","title":"My First Post"},{"content":"","href":"/authors/muniftanjim/","title":"muniftanjim"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/tags/config/","title":"Configuration"},{"content":"","href":"/tags/og/","title":"Opengraph"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"}]
